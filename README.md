# seq2seq
Just a place to dump seq2seq stuff I've been getting inspirations from for my own research

#### Machine Translation [Sequence to Sequence Learning with Neural Networks](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) NIPS 2014
Introduced Sequence to Sequence learning using an Encoder-Decoder structure and LSTMs.

#### Blog [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) 2015
Fun, informative writeup on RNNs.

#### Machine Translation [Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122) 2017
From Facebook AI Research. Replaces RNNs with Convolutions.

#### Machine Translation [Attention is All you Need](https://arxiv.org/abs/1706.03762) NIPS 2017
No RNNs, no convs, just attention.

#### Blog [The fall of RNN/LSTM](https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0) 2018
:(

#### Misc. [An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/abs/1803.01271) 2018
According to paper TCN method scores much better than LSTM or GRU units on memory benchmark tests.

