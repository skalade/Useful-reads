# seq2seq
Just a place to dump useful seq2seq resources I've come across

## Papers

#### [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078), arXiv:1406.1078, EMNLP 2014

#### [Sequence to Sequence Learning with Neural Networks](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) arXiv:1409.3215, NIPS 2014

#### [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) arXiv:1409.0473, ICLR 2015

#### [A Neural Conversational Model](https://arxiv.org/abs/1506.05869) arXiv:1506.05869, 2015

#### [Listen, Attend and Spell](https://arxiv.org/abs/1508.01211) arXiv:1508.01211, ICASSP 2016

#### [Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/abs/1609.08144) arXiv:1609.08144, 2016

#### [Language Modeling with Gated Convolutional Networks](https://arxiv.org/abs/1612.08083) arXiv:1612.08083, 2016

#### [End to End Deep Neural Network Frequency Demodulation of Speech Signals](https://arxiv.org/abs/1704.02046)  	arXiv:1704.02046, 2017

#### [Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122) arXiv:1705.03122, 2017

#### [Attention is All you Need](https://arxiv.org/abs/1706.03762) arXiv:1706.03762, NIPS 2017

#### [Neural Network Detection of Data Sequences in Communication Systems](https://arxiv.org/abs/1802.02046) arXiv:1802.02046, 2018

#### [An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/abs/1803.01271) arXiv:1803.01271, 2018

## Blogs

#### [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) 2015

#### [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) 2015

#### [Why I Use raw_rnn Instead of dynamic_rnn in Tensorflow and So Should You](https://hanxiao.github.io/2017/08/16/Why-I-use-raw-rnn-Instead-of-dynamic-rnn-in-Tensorflow-So-Should-You-0/) 2017

#### [The fall of RNN/LSTM](https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0) 2018

#### [When Recurrent Models Don't Need to be Recurrent](http://www.offconvex.org/2018/07/27/approximating-recurrent/) 2018

#### [Attention? Attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) 2018

## Tutorials and code

#### [Neural Machine Translation (seq2seq) Tutorial](https://www.tensorflow.org/tutorials/seq2seq) from tensorflow.org

#### [Translation with a Sequence to Sequence Network and Attention](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) from pytorch.org

## Talks

#### [Can machine learning trump theory in communication system design?](https://www.youtube.com/watch?v=7L4PHaYP6O4) by Prof Andrea Goldsmith in Edinburgh Uni, 2018 (actually attended this one really good!)

#### [Seq2Seq ICML Tutorial](https://sites.google.com/view/seq2seq-icml17) by Oriol Vinyals @OriolVinyalsML & Navdeep Jaitly @NavdeepLearning, ICML 2017

#### [Nuts and Bolts of Applying Deep Learning](https://www.youtube.com/watch?v=7L4PHaYP6O4) by Andrew Ng, 2016

#### [CS231n Winter 2016: Lecture 10: Recurrent Neural Networks](https://www.youtube.com/watch?v=yCC09vCHzF8) by Andrej Karpathy, Stanford, 2016
