# seq2seq
Just a place to dump seq2seq stuff I've been getting inspirations from for my own research

#### [Sequence to Sequence Learning with Neural Networks](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) Machine Translation, NIPS 2014
Introduced Sequence to Sequence learning using an Encoder-Decoder structure and LSTMs.

#### [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) Blog, 2015
Fun, informative writeup on RNNs.

#### [Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122) Machine Translation, 2017
From Facebook AI Research. Replaces RNNs with Convolutions.

#### [Attention is All you Need](https://arxiv.org/abs/1706.03762) Machine Translation, NIPS 2017
No RNNs, no convs, just attention.

#### [The fall of RNN/LSTM](https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0) Blog, 2018
:(

#### [An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/abs/1803.01271) Misc., 2018
According to paper TCN method scores much better than LSTM or GRU units on memory benchmark tests.

